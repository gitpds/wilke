{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statement and loading environment variables\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "from google.cloud import storage \n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User set variables\n",
    "\n",
    "user = '5037547138'\n",
    "\n",
    "\n",
    "bot_name = 'wilke'\n",
    "# TODO - Can be used to start with a baseline prompt, then customize the bot on creation with the prompt. \n",
    "\n",
    "# prompt = TODO - add prompt, allowing the customerization of the bot on creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard variables and formulas for GCP\n",
    "\n",
    "working_dir = pathlib.Path.cwd()\n",
    "bucket_name = 'gpt_chat_logs'\n",
    "chat_file_folder = working_dir.joinpath('chat_logs')\n",
    "file_path = chat_file_folder.joinpath('{}_{}_chat.json'.format(user, bot_name))\n",
    "file_name = ('{}_{}_chat.json'.format(user, bot_name))\n",
    "STORAGE_CLASSES = ('STANDARD', 'NEARLINE', 'COLDLINE', 'ARCHIVE')\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for working with Google Cloud Storage\n",
    "\n",
    "class GCStorage:\n",
    "    def __init__(self, storage_client):\n",
    "        self.client = storage_client\n",
    "\n",
    "    # Create a bucket\n",
    "    def create_bucket(self, bucket_name, storage_class, bucket_location='US'):\n",
    "        bucket = self.client.bucket(bucket_name)\n",
    "        bucket.storage_class = storage_class\n",
    "        return self.client.create_bucket(bucket, bucket_location)        \n",
    "\n",
    "    # Get a bucket\n",
    "    def get_bucket(self, bucket_name):\n",
    "        return self.client.get_bucket(bucket_name)\n",
    "\n",
    "    # List all buckets\n",
    "    def list_buckets(self):\n",
    "        buckets = self.client.list_buckets()\n",
    "        return [bucket.name for bucket in buckets]\n",
    "\n",
    "    # Upload a file to a bucket\n",
    "    def upload_file(self, bucket, blob_destination, file_path):\n",
    "        blob = bucket.blob(blob_destination)\n",
    "        blob.upload_from_filename(file_path, content_type='application/json')\n",
    "        return blob\n",
    "\n",
    "    # List all blobs in a bucket\n",
    "    def list_blobs(self, bucket_name):\n",
    "        return self.client.list_blobs(bucket_name)\n",
    "    \n",
    "    def download_blob(self, bucket_name, blob_name, file_path):\n",
    "        bucket = self.client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        blob.download_to_filename(file_path)\n",
    "        return blob\n",
    "    \n",
    "    \n",
    "    def load_file(self, bucket, blob_name):\n",
    "        bucket = storage_client.get_bucket(bucket)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        data = json.loads(blob.download_as_string(client=None))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the GCP storage class and loading the chat log file\n",
    "gcs = GCStorage(storage_client)\n",
    "CHAT_LOG_FILE = gcs.load_file(bucket_name, file_name)\n",
    "chat_log = CHAT_LOG_FILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bucket if it doesn't exist and chat log management functions\n",
    "\n",
    "def get_or_create_bucket(bucket_name):\n",
    "    if not bucket_name in gcs.list_buckets():\n",
    "        gcs.create_bucket(bucket_name, STORAGE_CLASSES[0])\n",
    "    else:\n",
    "        gcs.get_bucket(bucket_name)\n",
    "        \n",
    "        \n",
    "# Create a new chat log file in the local chat_logs folder\n",
    "def create_chat_log_file(user, bot_name):\n",
    "    # TODO This function requires me to manually load the starting json \n",
    "    # Opportunity to integrate the startup exeperience while solving this manual operation\n",
    "    filename = f'{user}_{bot_name}_chat.json'\n",
    "    file_path = chat_file_folder.joinpath(filename)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write('')\n",
    "    return file_path\n",
    "\n",
    "# upload the chat log file to the bucket\n",
    "def upload_chat_log_file(bucket_name, file_path):\n",
    "    name = f'{user}_{bot_name}_chat.json'\n",
    "    filename = name\n",
    "    bucket = gcs.get_bucket(bucket_name)\n",
    "    blob = gcs.upload_file(bucket, filename, file_path)\n",
    "    return blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================\n",
    "## Called functions that will work correctly\n",
    "\n",
    "# create_chat_log_file(user, bot_name)\n",
    "# upload_chat_log_file(bucket_name, file_path)\n",
    "# data = gcs.load_file(bucket_name, file_name)\n",
    "# print(data  )\n",
    "\n",
    "#========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================\n",
    "# Test function to make sure the model is returning a response\n",
    "\n",
    "# completion = openai.ChatCompletion.create(\n",
    "#   model=\"gpt-3.5-turbo\",\n",
    "#   messages = [\n",
    "#             {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#             {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "#             {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "#             {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "#              ] \n",
    "#             )\n",
    "\n",
    "# print(completion.choices[0].message)\n",
    "#========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle the chat log and return the response\n",
    "\n",
    "def text(question):\n",
    "    bucket = gcs.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "\n",
    "\n",
    "        messages = chat_log)\n",
    "    question = str(question)\n",
    "    return_response = (response)['choices'][0]['message']['content']\n",
    "    \n",
    "    new_log = chat_log\n",
    "    new_log.append({\"role\": \"user\", \"content\": question})\n",
    "    new_log.append({\"role\": \"assistant\", \"content\": return_response})\n",
    "    \n",
    "    json_data = json.dumps(new_log, indent=4)\n",
    "    \n",
    "    # print(json_data)\n",
    "    # print(new_log)\n",
    "\n",
    "    blob.upload_from_string(json_data, content_type='application/json')\n",
    "\n",
    "    return return_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test Question\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAre you going to finally work?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m, in \u001b[0;36mtext\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      4\u001b[0m bucket \u001b[38;5;241m=\u001b[39m gcs\u001b[38;5;241m.\u001b[39mget_bucket(bucket_name)\n\u001b[1;32m      5\u001b[0m blob \u001b[38;5;241m=\u001b[39m bucket\u001b[38;5;241m.\u001b[39mblob(file_name)\n\u001b[0;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchat_log\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(question)\n\u001b[1;32m     12\u001b[0m return_response \u001b[38;5;241m=\u001b[39m (response)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pdsai_env/lib/python3.8/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "# Test Question\n",
    "text('Are you going to finally work?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text for conversation generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdsai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
